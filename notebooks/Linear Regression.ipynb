{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how simple it is to implement a linear regression in python.\n",
    "\n",
    "1) We need some data so create an array x with shape (100,1) that contains the numbers from -5 to 5.\n",
    "\n",
    "2) Initialize the random number generator of numpy to 42.\n",
    "\n",
    "3) Now add Gaussian random noise to x with a standard deviation of 1 and store the result in an array y.\n",
    "\n",
    "4) Plot the datapoints as yellow dots in the range $x\\in[-10,10]$ and $y\\in[-10,10]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression for datapoint matrix $X$ ($D \\times N$, D datapoints and N input dimensions) and target matrix $Y$ ($D \\times M$, D datapoints and M output dimensions) is defined as:\n",
    "\n",
    "$min \\frac{1}{2}\\left(A\\vec{x}-\\vec{y}\\right)^2$\n",
    "\n",
    "We ignore the bias value for now!\n",
    "\n",
    "1) Set the derivative to zero and solve the equation for $A$ to get the optima $A^*$. (Hint: If you have problems with the closed form solution with abitray dimensions, first solve the equation for 1D input and output)\n",
    "\n",
    "2) Now plot the resulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A commen way to integrate a bias value for many machine learning methods is to add a dimension which is constant one for all datapoints!\n",
    "\n",
    "1) Modify the code by adding a second constant dimension to x and add 10 to y to shift the datapoints verctically.\n",
    "\n",
    "2) Now plot the result in the range $x\\in[-10,10]$ and $y\\in[0,20]$, notice that you have to select the first dimension of x in order not to plot the constant dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a polynomial expansion of x we can fit a polynome to the data.\n",
    "\n",
    "Fit a ploynome of degree 5 to the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the same using the linear regression function np.polyfit(x,y,5) of numpy. Notice that x,y are 1D arrays here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
