{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Profiling with IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes our scripts take a lot of time or memory to run. That happens especially for machine learning tasks when we train our algorithms on a large data sets. Profiling a program is a way of finding out where it becomes slow and where exactly its memory bottlenecks are. Luckily, IPython makes it very easy to profile a python script.\n",
    "\n",
    "Let's assume we have a few functions that different calculations. We define them here directly but they could also come from your .py file and are imported into IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def foo():\n",
    "    print 'foo: calculating heavy stuff...'\n",
    "    sleep(1)\n",
    "\n",
    "def bar():\n",
    "    print 'bar: calculating heavy stuff...'\n",
    "    sleep(2)\n",
    "\n",
    "def baz():\n",
    "    foo()\n",
    "    bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython magic function\n",
    "\n",
    "IPython provides a few so-called [*magic* functions](https://ipython.org/ipython-doc/3/interactive/magics.html) that start with `%` and are not part of the Python language. For profiling there are [`%time`](https://ipython.org/ipython-doc/dev/interactive/magics.html#magic-time) and [`%timeit`](https://ipython.org/ipython-doc/dev/interactive/magics.html#magic-timeit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo: calculating heavy stuff...\n",
      "bar: calculating heavy stuff...\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%time baz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo: calculating heavy stuff...\n",
      "bar: calculating heavy stuff...\n",
      "foo: calculating heavy stuff...\n",
      "bar: calculating heavy stuff...\n",
      "foo: calculating heavy stuff...\n",
      "bar: calculating heavy stuff...\n",
      "foo: calculating heavy stuff...\n",
      "bar: calculating heavy stuff...\n",
      "1 loops, best of 3: 3.01 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit baz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, [`%time`](https://ipython.org/ipython-doc/dev/interactive/magics.html#magic-time) runs a function and measures its execution time. [`%timeit`](https://ipython.org/ipython-doc/dev/interactive/magics.html#magic-timeit) runs a function several times and outputs statistics about the execution times.\n",
    "\n",
    "## Line-wise profiling\n",
    "\n",
    "Very often though we have the know perfectly well that our fuction `foo` is slow, we just don't know which party exactly make it that slow. That's where we can load the [line_profiler](https://pypi.python.org/pypi/line_profiler/) extension and output statistics about every single line.\n",
    "\n",
    "**Usage:**\n",
    "`%lprun -f function_to_profile function_to_call()`\n",
    "\n",
    "Let's call `baz()` and output the details for `foo()` and `baz()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "foo: calculating heavy stuff...\n",
      "bar: calculating heavy stuff...\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f foo -f baz baz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Memory profiling works very similar. Unfortunately, we don't have necessary dependencies met on the Windows machines here. We still show one example how it *could* look like.\n",
    "\n",
    "Let's start with a function that uses a bit of memory:\n",
    "\n",
    "```python\n",
    "def moo():\n",
    "    a = [1] * (10 ** 6)\n",
    "    b = [2] * (2 * 10 ** 7)\n",
    "    del b\n",
    "    return a\n",
    "```\n",
    "\n",
    "Again there is an IPython magic function called `%memit`:\n",
    "\n",
    "```python\n",
    "%memit moo()\n",
    "```\n",
    "\n",
    "And there is a [memoriy_profiler](https://pypi.python.org/pypi/memory_profiler) module similar to [line_profiler](https://pypi.python.org/pypi/line_profiler/). However, it only works if the function to profile is a file, which is why we can't run it here in the Notebook. Running the folling code ...\n",
    "\n",
    "```python\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from my_module.py import moo\n",
    "\n",
    "%mprun -f moo moo()```\n",
    "\n",
    "... would look like this:\n",
    "\n",
    "> ```Line      Mem usage  Increment   Line Contents```\n",
    "> \n",
    ">      4      5.97 MB    0.00 MB   def my_func():\n",
    ">      5     13.61 MB    7.64 MB       a = [1] * (10 ** 6)\n",
    ">      6    166.20 MB  152.59 MB       b = [2] * (2 * 10 ** 7)\n",
    ">      7     13.61 MB -152.59 MB       del b\n",
    ">      8     13.61 MB    0.00 MB       return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "We prepared an example programm that reads a text corpus of one million words and calculates some simple statistics for this text. First it creates a list of unique words and second it counts how often each of these words occur. First some imports (NLTK is *Natural Language Toolkit*)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\mielecqs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "# download Brown corpus if neccessary\n",
    "nltk.download('brown')\n",
    "# make all words from corpus lower-case\n",
    "brown = [word.lower() for word in nltk.corpus.brown.words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the following code and get yourself a cup of coffee..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_statistics_slow(corpus):\n",
    "\n",
    "    # data structures for counting words\n",
    "    word_list = []\n",
    "    counter_list = []\n",
    "\n",
    "    # calculate statistics of words\n",
    "    for word in corpus:\n",
    "    \n",
    "        if word in word_list:\n",
    "            index = word_list.index(word)\n",
    "            counter_list[index] = counter_list[index] + 1\n",
    "        else:\n",
    "            word_list.append(word)\n",
    "            counter_list.append(1)\n",
    "            \n",
    "    # create a list of indices sorted according to word occurrances\n",
    "    sorted_indices = np.argsort(counter_list)\n",
    "\n",
    "    # only keep last ten entries of the index list\n",
    "    sorted_indices = sorted_indices[-10:]\n",
    "\n",
    "    # switch order of list\n",
    "    sorted_indices = sorted_indices[::-1]\n",
    "\n",
    "    print 'most common words:'\n",
    "    for index in sorted_indices:\n",
    "        print '%s (%d)' % (word_list[index], counter_list[index])\n",
    "    \n",
    "    print '\\nOverall there are %d unique words in the corpus.' % len(word_list)\n",
    "    \n",
    "# statistics for whole corpus (may take 15 minutes)\n",
    "print_statistics_slow(brown)\n",
    "\n",
    "# statistics for 100.000 first words only\n",
    "#print_statistics_slow(brown[:100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, nice. But what went wrong that the script became so **slooow**?!? Use the `line_profiler` to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# profiling for whole corpus (15 minutes again)\n",
    "%lprun -f print_statistics_slow print_statistics_slow(brown)\n",
    "\n",
    "# profiling for first 100.000 words only\n",
    "#%lprun -f print_statistics_slow print_statistics_slow(brown[:100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we have used the data structures in a very naive and inefficient way. Re-write the method that it runs faster. `line_profiler` may help you to gradually remove bottlenecks. In the end the method shouldn't need more than two or three seconds.\n",
    "\n",
    "> **Hint:** Think about using data structures `dict` and `set` instead of lists when appropriate. A `set` allows membership testing much more efficiently than a `list`. With a `dict` you can quickly map from a key to a value, for instance: word -> index.\n",
    "\n",
    "> Also this article about sorting may be usefull:\n",
    "> https://wiki.python.org/moin/HowTo/Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words:\n",
      "the (69971)\n",
      ", (58334)\n",
      ". (49346)\n",
      "of (36412)\n",
      "and (28853)\n",
      "to (26158)\n",
      "a (23195)\n",
      "in (21337)\n",
      "that (10594)\n",
      "is (10109)\n",
      "\n",
      "Overall there are 49815 unique words in the corpus.\n"
     ]
    }
   ],
   "source": [
    "def print_statistics_fast(corpus):\n",
    "    # data structures for counting words\n",
    "    word_list = {}\n",
    "\n",
    "    # calculate statistics of words\n",
    "    for word in corpus:\n",
    "        try:\n",
    "            word_list[word] += 1\n",
    "        except KeyError:\n",
    "            word_list[word] = 1\n",
    "            \n",
    "    # create a list of indices sorted according to word occurrances\n",
    "    sorted_indices = sorted(word_list.items(), key=lambda (k,v): v)\n",
    "\n",
    "    # only keep last ten entries of the index list\n",
    "    sorted_indices = sorted_indices[-10:]\n",
    "\n",
    "    # switch order of list\n",
    "    sorted_indices = sorted_indices[::-1]\n",
    "\n",
    "    print('most common words:')\n",
    "    for word, count in sorted_indices:\n",
    "        print '%s (%d)' % (word, count)\n",
    "    print '\\nOverall there are %d unique words in the corpus.' % len(word_list)\n",
    "\n",
    "%lprun -f print_statistics_fast print_statistics_fast(brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from solutions import *\n",
    "decrypt_solution(solution_profiling, passphrase='foo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
